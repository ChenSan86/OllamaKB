import os
import torch
import hashlib
import shutil
from fastapi import FastAPI, Request
from pydantic import BaseModel
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import asyncio
from typing import List, Dict, Any
from fastapi.middleware.cors import CORSMiddleware

MODEL_NAME = "deepseek-r1:7b"
DATA_PATH = "./documents/furina.txt"
CHROMA_DB_PATH = "./chroma_db"
EMBEDDINGS_CACHE = "./embeddings_cache"
EMBEDDING_MODEL = "GanymedeNil/text2vec-large-chinese"

def file_md5(filepath):
    with open(filepath, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

def load_document(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    return loader.load()

def load_all_documents(folder_path):
    docs = []
    print("\nğŸ“š æ­£åœ¨éå†åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶å¤¹: {}".format(folder_path))
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith('.txt'):
                file_path = os.path.join(root, file)
                print(f"  - è½½å…¥: {os.path.relpath(file_path, folder_path)}")
                loader = TextLoader(file_path, encoding="utf-8")
                docs.extend(loader.load())
    print(f"âœ… å…±åŠ è½½æ–‡æœ¬å—: {len(docs)}\n")
    return docs

def split_documents(documents, chunk_size=500, chunk_overlap=100):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼Ÿ", "ï¼", " ", ""]
    )
    return text_splitter.split_documents(documents)

def init_embeddings():
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
        cache_folder=EMBEDDINGS_CACHE
    )

def folder_md5(folder_path):
    import hashlib
    md5 = hashlib.md5()
    file_count = 0
    for root, _, files in os.walk(folder_path):
        for file in sorted(files):
            if file.lower().endswith('.txt'):
                file_count += 1
                file_path = os.path.join(root, file)
                with open(file_path, "rb") as f:
                    while True:
                        chunk = f.read(4096)
                        if not chunk:
                            break
                        md5.update(chunk)
    print(f"ğŸ”‘ æ–‡æ¡£æ€»æ•°: {file_count}ï¼ŒçŸ¥è¯†åº“MD5: {md5.hexdigest()}")
    return md5.hexdigest()

def create_or_load_vector_db(docs, embeddings, db_path, folder_path="./documents"):
    hash_path = os.path.join(db_path, "doc.md5")
    cur_md5 = folder_md5(folder_path)
    old_md5 = None
    if os.path.exists(hash_path):
        with open(hash_path, "r") as f:
            old_md5 = f.read().strip()
    if os.path.exists(db_path) and old_md5 == cur_md5:
        print("ğŸŸ¢ æ£€æµ‹åˆ°çŸ¥è¯†åº“æœªå˜æ›´ï¼Œç›´æ¥åŠ è½½å‘é‡æ•°æ®åº“ã€‚\n")
        return Chroma(persist_directory=db_path, embedding_function=embeddings)
    if os.path.exists(db_path):
        print("ğŸŸ¡ æ£€æµ‹åˆ°çŸ¥è¯†åº“å˜æ›´ï¼Œæ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“â€¦â€¦")
        shutil.rmtree(db_path)
    print("ğŸ”µ æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“â€¦â€¦")
    vector_db = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=db_path
    )
    os.makedirs(db_path, exist_ok=True)
    with open(hash_path, "w") as f:
        f.write(cur_md5)
    print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼\n")
    return vector_db

def init_llm(model_name=MODEL_NAME, base_url="http://localhost:11434"):
    return Ollama(
        model=model_name,
        base_url=base_url,
        temperature=0.0,
        verbose=False
    )

def create_rag_chain(llm, vector_db, prompt_template=None):
    if prompt_template is None:
        prompt_template = (
            "ä½ æ˜¯ä¸€ä¸ªå±±ä¸œå¤§å­¦çŸ¥è¯†åº“çš„ä¸“å®¶åŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\n"
            "å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç®€æ´ã€å‡†ç¡®åœ°æå–ç­”æ¡ˆï¼›å¦‚æœæ²¡æœ‰ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·æ— æ³•å›ç­”ã€‚\n\n"
            "ä¸Šä¸‹æ–‡: {context}\né—®é¢˜: {question}\n\nå›ç­”:"
        )
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    retriever = vector_db.as_retriever(search_kwargs={"k": 5})
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

class ChatRequest(BaseModel):
    question: str

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = MODEL_NAME
    messages: List[Message]
    temperature: float = 0.0
    stream: bool = False
    max_tokens: int = 1024

# åˆå§‹åŒ–RAGé“¾ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œæå‡æ€§èƒ½ï¼‰
documents = load_all_documents("./documents")
docs = split_documents(documents)
embeddings = init_embeddings()
vector_db = create_or_load_vector_db(docs, embeddings, CHROMA_DB_PATH, folder_path="./documents")
llm = init_llm()
# ä¸¥æ ¼æ¨¡å¼ï¼šåªå…è®¸åŸºäºçŸ¥è¯†åº“å›ç­”
def strict_prompt():
    return (
        "ä½ æ˜¯ä¸€ä¸ªå±±ä¸œå¤§å­¦çŸ¥è¯†åº“çš„ä¸“å®¶åŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\n"
        "å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç®€æ´ã€å‡†ç¡®åœ°æå–ç­”æ¡ˆï¼›å¦‚æœæ²¡æœ‰ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·æ— æ³•å›ç­”ã€‚\n\n"

        "ä¸Šä¸‹æ–‡: {context}\né—®é¢˜: {question}\n\nå›ç­”:"
    )
# è‡ªç”±æ¨¡å¼ï¼šå…è®¸AIè‡ªç”±å‘æŒ¥
def free_prompt():
    return (
        "ä½ æ˜¯ä¸€ä¸ªå±±ä¸œå¤§å­¦çŸ¥è¯†åº“çš„ä¸“å®¶åŠ©æ‰‹ã€‚è¯·ä¼˜å…ˆæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥æ ¹æ®ä½ è‡ªå·±çš„çŸ¥è¯†å’Œå¸¸è¯†è¿›è¡Œåˆç†æ¨æµ‹å’Œè¡¥å……ã€‚\n"
        "ä¸Šä¸‹æ–‡: {context}\né—®é¢˜: {question}\n\nå›ç­”:"
    )
strict_rag_chain = create_rag_chain(llm, vector_db, prompt_template=strict_prompt())
free_rag_chain = create_rag_chain(llm, vector_db, prompt_template=free_prompt())

@app.post("/chat")
async def chat(req: ChatRequest):
    question = req.question.strip()
    if not question:
        async def error_stream():
            yield "è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚"
        return StreamingResponse(error_stream(), media_type="text/plain")
    # é»˜è®¤ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼
    rag_chain = strict_rag_chain
    async def answer_stream():
        result = rag_chain({"query": question})
        answer = result['result']
        # æŒ‰æ®µè½æˆ–å¥å­æµå¼è¾“å‡º
        for chunk in answer.split("\n"):
            if chunk.strip():
                yield chunk + "\n"
                await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿæµå¼æ•ˆæœ
    return StreamingResponse(answer_stream(), media_type="text/plain")

@app.post("/v1/chat/completions")
async def openai_chat_completions(req: ChatCompletionRequest):
    # æ‹¼æ¥æ‰€æœ‰messagesä¸ºä¸Šä¸‹æ–‡
    context = ""
    for m in req.messages:
        if m.role == "system":
            context += f"[ç³»ç»Ÿè®¾å®š]{m.content}\n"
        elif m.role == "user":
            context += f"[ç”¨æˆ·]{m.content}\n"
        elif m.role == "assistant":
            context += f"[åŠ©æ‰‹]{m.content}\n"
    # å–æœ€åä¸€æ¡userä¸ºé—®é¢˜
    user_message = next((m for m in reversed(req.messages) if m.role == "user"), None)
    if not user_message:
        return JSONResponse(content={"error": {"message": "No user message provided."}}, status_code=400)
    question = user_message.content.strip()
    if not question:
        return JSONResponse(content={"error": {"message": "Empty question."}}, status_code=400)
    # æ ¹æ®modelå‚æ•°é€‰æ‹©é“¾
    model_name = req.model or "deepseek-r1:7b"
    use_free = (model_name.endswith(":free") or model_name.endswith("-free"))
    rag_chain = free_rag_chain if use_free else strict_rag_chain
    def get_openai_response():
        result = rag_chain({"query": question, "context": context})
        answer = result['result']
        return {
            "id": "chatcmpl-xxx",
            "object": "chat.completion",
            "created": int(__import__('time').time()),
            "model": req.model,
            "choices": [
                {
                    "index": 0,
                    "message": {"role": "assistant", "content": answer},
                    "finish_reason": "stop"
                }
            ]
        }
    if req.stream:
        async def stream_gen():
            result = rag_chain({"query": question, "context": context})
            answer = result['result']
            for chunk in answer.split("\n"):
                if chunk.strip():
                    data = {
                        "id": "chatcmpl-xxx",
                        "object": "chat.completion.chunk",
                        "created": int(__import__('time').time()),
                        "model": req.model,
                        "choices": [
                            {
                                "delta": {"role": "assistant", "content": chunk+"\n"},
                                "index": 0,
                                "finish_reason": None
                            }
                        ]
                    }
                    yield f"data: {__import__('json').dumps(data, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)
            # ç»“æŸä¿¡å·
            yield "data: [DONE]\n\n"
        return StreamingResponse(stream_gen(), media_type="text/event-stream")
    else:
        return JSONResponse(content=get_openai_response())

if __name__ == "__main__":
    print("\n================= Ollama RAG Server å¯åŠ¨ =================")
    print("APIåœ°å€: http://0.0.0.0:8000  (æœ¬æœºå¯ç”¨ http://127.0.0.1:8000)")
    print("çŸ¥è¯†åº“ç›®å½•: ./documents\n")
    uvicorn.run("main_server:app", host="0.0.0.0", port=8000, reload=False)
